{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import *\n",
    "y_tr, x_tr, y_te, x_te = split_data(tX, y, 0.8, seed=20)\n",
    "\n",
    "undefined_features = [[4, 5, 6, 12, 22, 23, 24, 25, 26,\n",
    "                       27, 28, 29], [4, 5, 6, 12, 22, 26, 27, 28], [22], [22]]\n",
    "PRI_jet_num = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test set into jets according to above undefined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_train, y_jet_train, index = get_jets(x_tr, y_tr, PRI_jet_num, undefined_features, list_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_test, y_jet_test, index_te = get_jets(x_te, y_te, PRI_jet_num, undefined_features, list_ = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing : standardization, normalisation, removal of correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See data_preprocessing.py for further documentation\n",
    "\n",
    "**Screening range and input parameters** :\n",
    "- Least squares (normal equations): False; True; False (need standardization)\n",
    "- Least squares Gradient Descent: False; True; False\n",
    "- Least squares stochastic gradient descent: False; True; False\n",
    "- Ridge Regression: False; True; False\n",
    "- Logistic regression: True,False,False\n",
    "- Regularized Logistic regression: True,False,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import preprocessing_data\n",
    "jet_pr_tr = []\n",
    "jet_pr_te = []\n",
    "for jet in jet_train:\n",
    "    jet_pr_tr.append(preprocessing_data(jet, normalization = True, standardization = True, correl = True))\n",
    "    \n",
    "for jet in jet_test:\n",
    "    jet_pr_te.append(preprocessing_data(jet, normalization = True, standardization = True, correl = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning without regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find optimal degree across lambdas or optimal lambda across degrees. We've choosen to find optimal degree across lambda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods :**\n",
    "- **1** : Least squares\n",
    "- **2** : Least squares gradient descent (least squares GD)\n",
    "- **3** : Least squares stochastic gradient descent (least squares SGD)\n",
    "- **5** : Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Screening range and input parameters** :\n",
    "- *Least squares (normal equations):* degrees 1 to 20, standardized data\n",
    "  - accuracy = 0.77858\n",
    "  - best degrees for each jet: [4,4,9,9]\n",
    "- *Least squares gradient descent:* optimal degree was 1, with standardized data, 15000 iterations, gamma = 0.000001. No need for optimisation\n",
    "  - accuracy = 0.74346\n",
    "  - best degree for all jets : 1\n",
    "- *Least squares stochastic gradient descent:* standardized data, degrees 1 to 3  gamma = 0.000000001\n",
    "  - accuracy = 0.74274\n",
    "  - best parameters for all jets : [1,1,2,4]\n",
    "- *Logistic regression:* normalized (only) data, degrees = np.arange(5,10,1), lambdas = np.logspace(-20,-15,1), gamma = 0.000001\n",
    "  - accuracy on test set = 0.70682\n",
    "  - best parameters for all jets : best degree 9 with lambda 1e-20 2500 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning and predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import choose_your_methods\n",
    "from proj1_helpers import select_best_parameter, best_w\n",
    "from optimized_utils import *\n",
    "\n",
    "#remember best degrees for each jet\n",
    "best_degs = []\n",
    "\n",
    "#remember weight and predictions for each jet\n",
    "w_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for (jet, y_jet, index) in zip(jet_pr_tr, y_jet_train, np.arange(4)):\n",
    "    print(\"Jet n°\",index)\n",
    "    #find best parameter\n",
    "    best_degree, acc_te, _, _ = select_best_parameter(y_jet, jet, 1, 'degree', by_accuracy = True, seed = 1 , k_fold = 5, degrees = np.arange(1,20,1), lambdas = np.array([0]))\n",
    "    best_degs.append(best_degree)\n",
    "    \n",
    "    #find optimal weight\n",
    "    x_augm = build_poly(jet, best_degree)\n",
    "    w = best_w(y_jet , jet , 1, 0 , best_degree)\n",
    "    w_list.append(w)\n",
    "    \n",
    "    #compute prediction\n",
    "for(jet, w, deg) in zip(jet_pr_te, w_list, best_degs):\n",
    "    x_augm_te = build_poly(jet, deg)\n",
    "    y_pred = x_augm_te.dot(w)\n",
    "    y_pred_list.append(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimized_utils import accuracy\n",
    "y_predict = combine_jets(y_pred_list, index_te)\n",
    "print(\"accuracy on test set: \", accuracy(y_te, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods, screening range and parameters :**\n",
    "- **4**: *Ridge regression:* standardized (only) data, degrees = np.array([1,5,6,7,8,10,12]), lambdas = [0.009999, 0.1, 0.001]\n",
    "  - accuracy on test set = 0.82258, optimal degrees = [12] with lambdas = [0.001, 0.009999, 0.001, 0.009999]\n",
    "- **6**: *Regularized logistic regression:* normalized (only) data, degrees = 5 to 9, lambdas = np.logspace(-20,-15,1), gamma = 0.000001\n",
    "  - accuracy on test set = 0.65798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import select_best_parameter, best_w\n",
    "from optimized_utils import build_poly\n",
    "\n",
    "\n",
    "best_degs = []\n",
    "best_lbds = []\n",
    "w_list = []\n",
    "\n",
    "\n",
    "for (jet, y_jet, index) in zip(jet_pr_tr, y_jet_train, np.arange(4)):\n",
    "    print(\"Jet n°:\", index)\n",
    "    best_degree, rmse_te, best_lambda,_ = select_best_parameter(y_jet, jet, 6 , 'degree', by_accuracy=False, seed = 1, k_fold = 5, degrees = np.arange(5,9,1), lambdas = np.logspace(-25,-20,1), gamma = 0.000001)\n",
    "    best_lbds.append(best_lambda)\n",
    "    best_degs.append(best_degree)\n",
    "    \n",
    "    x_augm = build_poly(jet, best_degree)\n",
    "    w = best_w(y_jet , jet , 6, best_lambda , best_degree)\n",
    "    w_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for(jet, w, deg) in zip(jet_pr_te, w_list, best_degs):\n",
    "    x_augm_t = build_poly(jet, deg)\n",
    "    y_pred_ = x_augm_t.dot(w)\n",
    "    y_pred_list.append(y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = combine_jets(y_pred_list, index_te)\n",
    "print(\"Accuracy on test set:\", accuracy(y_te, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation plot example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge, jet 1 (processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import select_best_degree\n",
    "best_degree, rmse_te, best_lambda,_ = select_best_degree(y_jet_train[1], jet_pr_tr[1], 4, False, 200, 5, np.arange(1,7,1), np.logspace(-7,-2,3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy evaluation for different degrees with the best parameters determined by the parameters optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import accuracy_visualization\n",
    "#Least squares accuracies\n",
    "acc_LS = [0.70876, 0.74764, 0.74754, 0.74128, 0.71198, 0.68274, 0.69782, 0.54796, 0.4796, 0.62668, 0.46492, 0.56832, 0.47284, 0.57026]\n",
    "\n",
    "#Ridge regression accuracies\n",
    "acc_R = [0.7635, 0.78948, 0.7994, 0.8055, 0.80728, 0.80928, 0.81468, 0.82088, 0.823, 0.82204, 0.8232, 0.82524, 0.7631, 0.8148]\n",
    "\n",
    "#Logistic regression accuracies\n",
    "acc_Logi=[0.66536,0.66536,0.57554,0.66536,0.57554,0.66536,0.57554,0.66536,0.57554,0.66536,0.57554,0.66536,0.57554,0.66536]\n",
    "\n",
    "accuracy_visualization(acc_LS,acc_R,acc_Logi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
